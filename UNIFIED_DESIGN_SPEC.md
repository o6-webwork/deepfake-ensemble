# Unified Design Specification: Hybrid OSINT Deepfake Detection Platform

**Version:** 2.1
**Date:** December 11, 2025
**Updated:** December 11, 2025 (Added OSINT-specific enhancements)
**Language:** Python 3.10+
**Framework:** Streamlit
**Architecture Pattern:** "Cyborg" (Signal Processing + Semantic Reasoning)

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [OSINT-Specific Enhancements (NEW)](#osint-specific-enhancements)
4. [Feature 1: Forensic Image Scanner (ELA/FFT + Metadata)](#feature-1-forensic-image-scanner)
5. [Feature 2: Dynamic Model Configuration UI](#feature-2-dynamic-model-configuration-ui)
6. [Feature 3: Hybrid Classification System](#feature-3-hybrid-classification-system)
7. [Two-Stage Prompting Strategy (KV-Cache Optimization)](#two-stage-prompting-strategy)
8. [Implementation Modules](#implementation-modules)
9. [Data Flow](#data-flow)
10. [API Specifications](#api-specifications)
11. [UI/UX Specifications](#uiux-specifications)
12. [Security & Best Practices](#security--best-practices)
13. [Testing & Validation](#testing--validation)

---

## Overview

### Mission Statement

Build a **hybrid OSINT-specialized deepfake detection platform** that combines:
- **Forensic signal processing** (ELA, FFT, Metadata) for objective artifact detection
- **OSINT-aware semantic reasoning** with context-adaptive analysis
- **Vision-Language Models** as Senior OSINT Image Analysts
- **Logit calibration** for scientifically-calibrated confidence scores
- **KV-cache optimization** for latency reduction via two-stage prompting
- **Flexible model management** for hotswapping local/online models

### Key Differentiators

1. **OSINT-Specific Context Handling**: Automatic scene classification (Military/Disaster/Propaganda) with domain-adapted forensic thresholds
2. **Hybrid Analysis**: Signal processing (FFT/ELA) + Semantic reasoning (VLM Chain-of-Thought)
3. **Three-Tier Classification**: Authentic / Suspicious / Deepfake (instead of binary)
4. **Metadata Auto-Fail**: Instant rejection for known AI tool signatures (Midjourney, Stable Diffusion, etc.)
5. **Calibrated probabilities**: Extract raw logprobs for true confidence scores
6. **KV-Cache Optimization**: Two-stage prompting reduces latency by 70-90%
7. **Model-agnostic architecture**: Support any OpenAI-compatible API endpoint
8. **Watermark Mode**: Configurable handling of news logos vs AI watermarks

---

## Architecture

### High-Level System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Streamlit Application                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Tab 1:         â”‚  â”‚   Tab 2:         â”‚  â”‚   Tab 3:      â”‚ â”‚
â”‚  â”‚   Detection      â”‚  â”‚   Evaluation     â”‚  â”‚   Config      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Core Processing Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Forensics   â”‚  â”‚  Classifier  â”‚  â”‚  Model Manager      â”‚   â”‚
â”‚  â”‚  (ELA/FFT)   â”‚  â”‚  (Logit Cal) â”‚  â”‚  (Config Handler)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Model Endpoints (OpenAI-compatible)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  vLLM    â”‚  â”‚  OpenAI  â”‚  â”‚  Gemini  â”‚  â”‚  Custom APIs   â”‚ â”‚
â”‚  â”‚  (Local) â”‚  â”‚  (Cloud) â”‚  â”‚  (Cloud) â”‚  â”‚  (Any)         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

**Core Dependencies:**
```
streamlit>=1.28.0          # Web UI framework
pillow>=10.0.0             # Image processing
opencv-python-headless     # ELA/FFT generation
numpy>=1.24.0              # Numerical operations
pandas>=2.0.0              # Data manipulation
openai>=1.0.0              # OpenAI-compatible client
xlsxwriter>=3.1.0          # Excel export
```

**Optional Dependencies:**
```
google-generativeai        # Gemini API support
scikit-image               # Advanced ELA (if needed)
exifread                   # EXIF metadata extraction (NEW)
```

---

## OSINT-Specific Enhancements

### Overview

The system is specifically designed for OSINT (Open Source Intelligence) image analysis in military, disaster, and propaganda contexts. Unlike general deepfake detectors, this system accounts for domain-specific visual characteristics that would otherwise trigger false positives.

### Problem Statement

**Generic deepfake detectors fail in OSINT contexts because:**

1. **Military imagery** contains legitimate repetitive patterns (uniforms, formations) that FFT flags as AI artifacts
2. **Disaster footage** is inherently grainy and chaotic, triggering "high entropy" false positives
3. **State media** uses heavy post-processing (HDR, color grading) that mimics manipulation artifacts
4. **Watermarks** from news agencies are confused with AI generation watermarks

### Solution: Context-Adaptive Analysis

The system implements **three specialized analysis protocols** based on automatic scene classification:

#### CASE A: Military Context (Uniforms / Parades / Formations)

**Characteristics:**
- Repetitive geometric patterns (marching columns, uniform grid formations)
- Standardized equipment creating visual regularity

**Adaptive Filtering:**
- **FFT:** Ignore repetitive grid artifacts (expected from formations)
- **Focus:** Look for clone stamp errors (duplicate faces, identical dirt patches, floating weapons)
- **Threshold Adjustment:** Increase FFT peak threshold by +20% (24 instead of 20)

**Rationale:** Legitimate military formations create frequency domain patterns indistinguishable from GAN artifacts.

#### CASE B: Disaster/HADR Context (Flood / Rubble / Combat / BDA)

**Characteristics:**
- High-entropy noise from damaged infrastructure
- Grainy, low-light footage
- Motion blur and compression from emergency documentation

**Adaptive Filtering:**
- **FFT:** Ignore high-entropy noise (chaos is expected)
- **ELA:** Do NOT flag "messy" textures as suspicious
- **Focus:** Look for physics failures (liquid fire, smoke without shadows, debris blending into ground)

**Rationale:** Disaster zones are inherently chaotic; "clean" images are more suspicious than "messy" ones.

#### CASE C: Propaganda/Showcase Context (Studio / News / State Media)

**Characteristics:**
- Professional post-processing (sharpening, HDR, color grading)
- High production value
- Potential legitimate watermarks

**Adaptive Filtering:**
- **ELA:** Expect high contrast (post-processing is normal)
- **Focus:** Distinguish "beautification" (skin smoothing) from "generation" artifacts
- **Metadata:** Check for professional camera equipment signatures

**Rationale:** State media uses heavy editing; must distinguish professional post-production from AI generation.

### Watermark Handling

Two configurable modes:

**Ignore Mode (Default):**
- Treat all watermarks as potential OSINT source attributions
- Do NOT flag news agency logos (CNN, Reuters, AP) as AI indicators
- Rationale: Preserving source attribution is standard OSINT practice

**Analyze Mode:**
- Actively scan for known AI watermarks (Sora, NanoBanana, colored strips)
- Flag suspected AI watermarks separately from news logos
- Requires visual distinction between standard TV overlays and AI signatures

### Implementation Strategy

**Auto-Classification:**
1. VLM performs initial scene classification based on visual cues
2. System prompt dynamically injects appropriate protocol (Case A/B/C)
3. Forensic thresholds automatically adjust based on context

**Manual Override:**
- User can explicitly set scenario: `"auto"`, `"military"`, `"disaster"`, `"propaganda"`
- Allows expert analysts to force specific analysis protocols

---

## Two-Stage Prompting Strategy (KV-Cache Optimization)

### Overview

The system implements a **two-stage API calling pattern** to maximize Key-Value cache reuse, reducing latency by 70-90% and cost by similar margins for the verdict generation phase.

### Problem Statement

**Traditional Single-Call Approach:**
- Send system prompt + image + analysis prompt + verdict prompt in one call
- Model processes everything sequentially
- High latency (processing all images from scratch each time)
- High cost (all tokens billed every time)

**Two-Stage Approach:**
- **Request 1:** System prompt + Image + Analysis prompt â†’ Returns analysis history
- **Request 2:** Append verdict prompt to existing history â†’ Returns single-token verdict
- **Benefit:** Image tokens are cached in KV-store from Request 1, making Request 2 nearly instant

### Latency Constraint

**CRITICAL REQUIREMENT:**
The verdict API call (Request 2) **MUST be a continuation** of the analysis call (Request 1). This is enforced by:
1. Using the same conversation history object
2. Appending verdict prompt to existing messages array
3. Ensuring no context window reset between calls

### Implementation Pattern

#### Request 1: Analysis (Heavy Computation)

```python
# Construct initial message with all forensic images
messages = [
    {
        "role": "system",
        "content": OSINT_SYSTEM_PROMPT  # Includes CASE A/B/C protocols
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "--- FORENSIC LAB REPORT ---"},
            {"type": "text", "text": forensic_report},  # ELA/FFT/EXIF findings
            {"type": "text", "text": "--- ORIGINAL IMAGE ---"},
            {"type": "image_url", "image_url": {"url": original_image_uri}},
            {"type": "text", "text": "--- ELA MAP ---"},
            {"type": "image_url", "image_url": {"url": ela_image_uri}},
            {"type": "text", "text": "--- FFT SPECTRUM ---"},
            {"type": "image_url", "image_url": {"url": fft_image_uri}},
            {
                "type": "text",
                "text": (
                    "Analyze the above forensic evidence. "
                    "Provide your reasoning for whether this image is "
                    "authentic or AI-generated."
                )
            }
        ]
    }
]

# Request 1: Get analysis with reasoning
response_1 = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.0,
    max_tokens=300  # Allow detailed reasoning
)

# Extract assistant's analysis
analysis_text = response_1.choices[0].message.content

# Store assistant response in conversation history
messages.append({
    "role": "assistant",
    "content": analysis_text
})
```

**Cost/Latency:** ~2000 tokens (3 images + prompts) processed at full speed

#### Request 2: Verdict (KV-Cache Optimized)

```python
# Append verdict prompt to existing conversation
messages.append({
    "role": "user",
    "content": (
        "Based on your analysis, provide your final verdict:\n\n"
        "(A) Real (Authentic Capture)\n"
        "(B) Fake (AI Generated/Manipulated)\n\n"
        "Answer with ONLY the single letter A or B."
    )
})

# Request 2: Get single-token verdict with logprobs
response_2 = client.chat.completions.create(
    model=model_name,
    messages=messages,  # Same conversation history
    temperature=0.0,
    max_tokens=1,       # Single token: A or B
    logprobs=True,
    top_logprobs=5
)

# Extract logprobs for calibrated confidence
verdict_token = response_2.choices[0].logprobs.content[0].token
logprobs = response_2.choices[0].logprobs.content[0].top_logprobs
```

**Cost/Latency:** ~10 tokens (verdict prompt only), image tokens reused from cache

### Benefits Breakdown

| Metric | Traditional | Two-Stage | Improvement |
|--------|-------------|-----------|-------------|
| **Request 1 Latency** | N/A | ~2.5s | Baseline |
| **Request 2 Latency** | ~2.5s | ~0.25s | **90% reduction** |
| **Request 2 Cost** | ~2000 tokens | ~10 tokens | **99% reduction** |
| **Total Time** | ~2.5s | ~2.75s | Minimal overhead |
| **Transparency** | None | Full reasoning visible | Explainability âœ“ |

### Safety Thresholds

To prevent hallucination from cached artifacts, the system enforces:

1. **Max Cache Age:** 60 seconds (server-side KV-cache TTL)
2. **Max Context Length:** 8192 tokens (prevents cache overflow)
3. **Conversation Reset:** New analysis = new conversation (no history pollution)

### OSINT-Specific System Prompt Integration

The system prompt dynamically adapts based on auto-detected context:

```python
# Base OSINT identity
BASE_PROMPT = """You are a Senior OSINT Image Analyst specializing in
military, disaster, and propaganda imagery verification."""

# Context-specific protocols
CASE_A_PROTOCOL = """CASE A: Military Context (Uniforms/Parades/Formations)
- Filter: IGNORE repetitive grid artifacts in FFT (formations create patterns).
- Focus: Look for clone stamp errors (duplicate faces, floating weapons).
- Threshold: FFT peak threshold increased by +20%."""

CASE_B_PROTOCOL = """CASE B: Disaster/HADR Context (Flood/Rubble/Combat)
- Filter: IGNORE high-entropy noise (chaos is expected).
- Focus: Look for physics failures (liquid fire, smoke without shadows).
- Do NOT flag "messy" textures as suspicious."""

CASE_C_PROTOCOL = """CASE C: Propaganda/Showcase Context (Studio/News)
- Filter: Expect high ELA contrast (post-processing is normal).
- Focus: Distinguish "beautification" from "generation" artifacts.
- Check metadata for professional camera signatures."""

# Dynamic protocol injection
def get_osint_system_prompt(context: str) -> str:
    """
    Generate context-adaptive OSINT system prompt.

    Args:
        context: "auto", "military", "disaster", or "propaganda"

    Returns:
        Complete system prompt with appropriate protocol
    """
    protocol_map = {
        "military": CASE_A_PROTOCOL,
        "disaster": CASE_B_PROTOCOL,
        "propaganda": CASE_C_PROTOCOL
    }

    if context == "auto":
        # Include all protocols for VLM to auto-select
        protocols = f"{CASE_A_PROTOCOL}\n\n{CASE_B_PROTOCOL}\n\n{CASE_C_PROTOCOL}"
    else:
        protocols = protocol_map.get(context, CASE_A_PROTOCOL)

    return f"{BASE_PROMPT}\n\n{protocols}"
```

### Module Specification: `detector.py`

This module orchestrates the two-stage detection process:

```python
"""
Two-stage OSINT-aware deepfake detection with KV-cache optimization.
Coordinates forensic analysis, context injection, and verdict extraction.
"""

import math
from typing import Dict, Tuple
from openai import OpenAI
from forensics import ArtifactGenerator
import exifread
import io


class OSINTDetector:
    """Two-stage OSINT-specialized deepfake detector."""

    # Token lists for verdict parsing
    REAL_TOKENS = ['A', ' A', 'a', ' a']
    FAKE_TOKENS = ['B', ' B', 'b', ' b']

    # Metadata blacklist for auto-fail
    AI_TOOL_SIGNATURES = [
        'midjourney', 'stable diffusion', 'stablediffusion',
        'dall-e', 'dalle', 'comfyui', 'automatic1111',
        'invokeai', 'fooocus', 'sora', 'firefly'
    ]

    def __init__(
        self,
        base_url: str,
        model_name: str,
        api_key: str = "dummy",
        context: str = "auto"
    ):
        """
        Initialize OSINT detector.

        Args:
            base_url: OpenAI-compatible API endpoint
            model_name: Model identifier
            api_key: API key (default: "dummy" for vLLM)
            context: "auto", "military", "disaster", or "propaganda"
        """
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model_name = model_name
        self.context = context
        self.artifact_gen = ArtifactGenerator()

    def detect(
        self,
        image_bytes: bytes,
        debug: bool = False
    ) -> Dict:
        """
        Perform two-stage OSINT-aware deepfake detection.

        Args:
            image_bytes: Original image as PNG/JPEG bytes
            debug: If True, include detailed debug information

        Returns:
            {
                "tier": str ("Authentic" / "Suspicious" / "Deepfake"),
                "confidence": float (0.0-1.0),
                "reasoning": str (analysis from stage 1),
                "forensic_report": str (ELA/FFT/EXIF findings),
                "metadata_auto_fail": bool,
                "raw_logits": {"real": float, "fake": float},
                "verdict_token": str,

                # Debug info (only if debug=True)
                "debug": {
                    "system_prompt": str,
                    "exif_data": dict,
                    "ela_variance": float,
                    "fft_pattern": str,
                    "fft_peaks": int,
                    "context_applied": str,
                    "threshold_adjustments": dict,
                    "request_1_latency": float,
                    "request_2_latency": float,
                    "request_1_tokens": int,
                    "request_2_tokens": int,
                    "top_k_logprobs": list,
                    "kv_cache_hit": bool,
                    "total_pipeline_time": float
                }
            }
        """
        # Stage 0: Check metadata for auto-fail
        metadata_report, auto_fail = self._check_metadata(image_bytes)

        if auto_fail:
            return {
                "tier": "Deepfake",
                "confidence": 1.0,
                "reasoning": "AI generation tool detected in metadata",
                "forensic_report": metadata_report,
                "metadata_auto_fail": True,
                "raw_logits": {"real": -100.0, "fake": 0.0},
                "verdict_token": "B"
            }

        # Stage 1: Generate forensic artifacts
        ela_bytes, fft_bytes = self.artifact_gen.generate_both(image_bytes)

        # Build forensic report text
        forensic_report = self._build_forensic_report(
            metadata_report,
            ela_bytes,
            fft_bytes
        )

        # Stage 2: Two-stage API calls
        analysis, verdict_result = self._two_stage_classification(
            image_bytes,
            ela_bytes,
            fft_bytes,
            forensic_report
        )

        # Determine tier based on confidence
        tier = self._classify_tier(verdict_result["confidence"])

        return {
            "tier": tier,
            "confidence": verdict_result["confidence"],
            "reasoning": analysis,
            "forensic_report": forensic_report,
            "metadata_auto_fail": False,
            "raw_logits": verdict_result["raw_logits"],
            "verdict_token": verdict_result["token"]
        }

    def _check_metadata(self, image_bytes: bytes) -> Tuple[str, bool]:
        """
        Extract EXIF metadata and check for AI tool signatures.

        Returns:
            (metadata_report: str, auto_fail: bool)
        """
        try:
            tags = exifread.process_file(io.BytesIO(image_bytes))

            # Build metadata report
            report_lines = ["EXIF Metadata:"]
            auto_fail = False

            for tag_name, tag_value in tags.items():
                tag_str = str(tag_value).lower()
                report_lines.append(f"  {tag_name}: {tag_value}")

                # Check for AI tool signatures
                if any(sig in tag_str for sig in self.AI_TOOL_SIGNATURES):
                    auto_fail = True
                    report_lines.append(
                        f"  âš ï¸ AI TOOL DETECTED: {tag_value}"
                    )

            if not tags:
                report_lines.append("  (No EXIF data found)")

            return "\n".join(report_lines), auto_fail

        except Exception as e:
            return f"EXIF extraction failed: {str(e)}", False

    def _build_forensic_report(
        self,
        metadata_report: str,
        ela_bytes: bytes,
        fft_bytes: bytes
    ) -> str:
        """Generate human-readable forensic report."""
        # TODO: Add ELA variance and FFT peak detection metrics
        report = f"""
{metadata_report}

ELA Analysis:
  - Compression variance: [To be computed]
  - Uniformity score: [To be computed]

FFT Analysis:
  - Peak pattern type: [To be computed]
  - Grid artifact score: [To be computed]
"""
        return report.strip()

    def _two_stage_classification(
        self,
        original_bytes: bytes,
        ela_bytes: bytes,
        fft_bytes: bytes,
        forensic_report: str
    ) -> Tuple[str, Dict]:
        """
        Perform two-stage API calls for analysis + verdict.

        Returns:
            (analysis_text: str, verdict_result: Dict)
        """
        # Convert images to base64
        import base64
        original_uri = f"data:image/png;base64,{base64.b64encode(original_bytes).decode()}"
        ela_uri = f"data:image/png;base64,{base64.b64encode(ela_bytes).decode()}"
        fft_uri = f"data:image/png;base64,{base64.b64encode(fft_bytes).decode()}"

        # Build system prompt with context-adaptive protocol
        system_prompt = self._get_system_prompt()

        # Request 1: Analysis (with forensic evidence)
        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "--- FORENSIC LAB REPORT ---"},
                    {"type": "text", "text": forensic_report},
                    {"type": "text", "text": "--- ORIGINAL IMAGE ---"},
                    {"type": "image_url", "image_url": {"url": original_uri}},
                    {"type": "text", "text": "--- ELA MAP ---"},
                    {"type": "image_url", "image_url": {"url": ela_uri}},
                    {"type": "text", "text": "--- FFT SPECTRUM ---"},
                    {"type": "image_url", "image_url": {"url": fft_uri}},
                    {
                        "type": "text",
                        "text": (
                            "Analyze the above forensic evidence. "
                            "Provide your reasoning for whether this image is "
                            "authentic or AI-generated."
                        )
                    }
                ]
            }
        ]

        response_1 = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            temperature=0.0,
            max_tokens=300
        )

        analysis_text = response_1.choices[0].message.content

        # Append assistant response to history
        messages.append({"role": "assistant", "content": analysis_text})

        # Request 2: Verdict (KV-cache optimized)
        messages.append({
            "role": "user",
            "content": (
                "Based on your analysis, provide your final verdict:\n\n"
                "(A) Real (Authentic Capture)\n"
                "(B) Fake (AI Generated/Manipulated)\n\n"
                "Answer with ONLY the single letter A or B."
            )
        })

        response_2 = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            temperature=0.0,
            max_tokens=1,
            logprobs=True,
            top_logprobs=5
        )

        # Parse verdict logprobs
        verdict_result = self._parse_verdict(response_2)

        return analysis_text, verdict_result

    def _get_system_prompt(self) -> str:
        """Generate context-adaptive OSINT system prompt."""
        base = """You are a Senior OSINT Image Analyst specializing in military, disaster, and propaganda imagery verification."""

        case_a = """
CASE A: Military Context (Uniforms/Parades/Formations)
- Filter: IGNORE repetitive grid artifacts in FFT (formations create patterns).
- Focus: Look for clone stamp errors (duplicate faces, floating weapons).
- Threshold: FFT peak threshold increased by +20%."""

        case_b = """
CASE B: Disaster/HADR Context (Flood/Rubble/Combat)
- Filter: IGNORE high-entropy noise (chaos is expected).
- Focus: Look for physics failures (liquid fire, smoke without shadows).
- Do NOT flag "messy" textures as suspicious."""

        case_c = """
CASE C: Propaganda/Showcase Context (Studio/News)
- Filter: Expect high ELA contrast (post-processing is normal).
- Focus: Distinguish "beautification" from "generation" artifacts.
- Check metadata for professional camera signatures."""

        if self.context == "auto":
            protocols = f"{case_a}\n{case_b}\n{case_c}"
        elif self.context == "military":
            protocols = case_a
        elif self.context == "disaster":
            protocols = case_b
        else:
            protocols = case_c

        return f"{base}\n{protocols}"

    def _parse_verdict(self, response) -> Dict:
        """
        Parse verdict logprobs from API response.

        Returns:
            {
                "token": str,
                "confidence": float,
                "raw_logits": {"real": float, "fake": float}
            }
        """
        try:
            logprobs_content = response.choices[0].logprobs.content[0]
            token = logprobs_content.token
            top_logprobs = logprobs_content.top_logprobs

            score_real = -100.0
            score_fake = -100.0

            for logprob_obj in top_logprobs:
                t = logprob_obj.token
                lp = logprob_obj.logprob

                if t in self.REAL_TOKENS and score_real == -100.0:
                    score_real = lp
                elif t in self.FAKE_TOKENS and score_fake == -100.0:
                    score_fake = lp

            # Softmax normalization
            p_real = math.exp(score_real)
            p_fake = math.exp(score_fake)
            confidence_fake = p_fake / (p_fake + p_real)

            return {
                "token": token,
                "confidence": confidence_fake,
                "raw_logits": {"real": score_real, "fake": score_fake}
            }

        except Exception as e:
            return {
                "token": None,
                "confidence": 0.5,
                "raw_logits": {"real": -0.69, "fake": -0.69},
                "error": str(e)
            }

    def _classify_tier(self, confidence_fake: float) -> str:
        """
        Classify into three-tier system.

        Thresholds:
            - confidence_fake >= 0.90 â†’ Deepfake
            - 0.50 <= confidence_fake < 0.90 â†’ Suspicious
            - confidence_fake < 0.50 â†’ Authentic
        """
        if confidence_fake >= 0.90:
            return "Deepfake"
        elif confidence_fake >= 0.50:
            return "Suspicious"
        else:
            return "Authentic"
```

### Usage Example

```python
# Initialize detector with OSINT context
detector = OSINTDetector(
    base_url="http://localhost:8000/v1/",
    model_name="Qwen3-VL-32B-Instruct",
    context="auto"  # Auto-detect Military/Disaster/Propaganda
)

# Read image
with open("test_image.jpg", "rb") as f:
    image_bytes = f.read()

# Perform two-stage detection
result = detector.detect(image_bytes)

print(f"Tier: {result['tier']}")  # Authentic / Suspicious / Deepfake
print(f"Confidence: {result['confidence']:.2%}")
print(f"Reasoning: {result['reasoning']}")
print(f"Forensic Report:\n{result['forensic_report']}")
```

---

## Feature 1: Forensic Image Scanner (ELA/FFT + Metadata)

### Rationale

**Problem:** Current VLM-based detection relies on subjective visual assessment without explicit forensic evidence.

**Solution:** Generate and provide **Error Level Analysis (ELA)**, **Fast Fourier Transform (FFT)**, and **EXIF metadata extraction** to the VLM, transforming it from a "visual guesser" to a "forensic signal interpreter."

### âš ï¸ Critical Forensic Pipeline Fixes (December 12, 2025)

Based on forensic expert review, the following critical failures were identified and remediated:

#### Fix 1: FFT Destructive Resizing â†’ Center Crop + Padding

**Problem**: Using `cv2.resize()` with `INTER_LINEAR` acts as a low-pass filter, mathematically smoothing out pixel-level AI generation artifacts (checkerboard patterns from Transpose Convolutions).

**Root Cause**: Linear interpolation destroys high-frequency evidence we're trying to detect.

**Solution**:
- **NEVER resize** - Use center crop instead
- **If image > 512Ã—512**: Extract center 512Ã—512 square
- **If image < 512Ã—512**: Pad with `cv2.BORDER_REFLECT` to 512Ã—512

```python
# BEFORE (WRONG - Destroys Evidence)
gray = cv2.resize(gray, (512, 512), interpolation=cv2.INTER_LINEAR)

# AFTER (CORRECT - Preserves Evidence)
h, w = gray.shape
crop_size = 512

# Center crop for large images
if h >= crop_size and w >= crop_size:
    start_y = (h - crop_size) // 2
    start_x = (w - crop_size) // 2
    gray = gray[start_y:start_y+crop_size, start_x:start_x+crop_size]
else:
    # Pad small images with reflection
    pad_y = max(0, crop_size - h)
    pad_x = max(0, crop_size - w)
    gray = cv2.copyMakeBorder(gray, 0, pad_y, 0, pad_x, cv2.BORDER_REFLECT)
```

#### Fix 2: Unmasked Social Media Artifacts â†’ DC + Axis Masking

**Problem**: Social media images (Twitter/Telegram) have sharp rectangular borders that create massive high-energy white crosses (+) in the FFT spectrum, 100Ã— stronger than any AI artifact. VLM fixates on the cross instead of actual anomalies.

**Root Cause**: Unmasked central DC component and axes create "JPEG Cross" artifact.

**Solution**: Mechanically mask (zero out):
1. **DC Component**: Central dot (5-pixel radius circle)
2. **Central Axes**: Horizontal and vertical lines through center (1-2 pixels wide)

```python
# Apply after log transform, before normalization
rows, cols = magnitude_log.shape
crow, ccol = rows // 2, cols // 2

# Mask DC component (center dot)
cv2.circle(magnitude_log, (ccol, crow), 5, 0, -1)

# Mask axes (the cross)
magnitude_log[crow-1:crow+2, :] = 0  # Horizontal axis
magnitude_log[:, ccol-1:ccol+2] = 0  # Vertical axis
```

#### Fix 3: ELA Variance Misinterpretation â†’ Context-Aware Thresholds

**Problem**: Documentation claims "Low variance (<2.0) = AI indicator" but platforms like WhatsApp/Facebook aggressively re-compress all images, causing real photos to have variance ~0.5.

**Root Cause**: Single global threshold fails on social media imagery.

**Solution**:
- **Update docstrings**: Clarify that low variance is **inconclusive** on social media
- **VLM guidance**: Look for **local inconsistencies** (e.g., bright patch on dark background), not global uniformity
- **Context-aware**: Social media images should not auto-fail on low ELA variance alone

```python
# Updated docstring
"""
ELA Variance Interpretation:
- Low variance (<2.0): INCONCLUSIVE on social media (re-compression artifacts)
- High variance (â‰¥2.0): Potential manipulation
- VLM should focus on LOCAL inconsistencies, not global uniformity
"""
```

#### Fix 4: Ambiguous Grid Instructions â†’ Macro vs Micro Distinction

**Problem**: Military context prompt says "IGNORE repetitive grid artifacts" which is too broad - teaches model to ignore GAN artifacts (which also look like grids).

**Root Cause**: No distinction between macro-scale formations and micro-scale pixel grids.

**Solution**: Explicitly differentiate:
- **Macro-Repetition** (Safe): Organic, imperfect alignment, low frequency (soldier formations)
- **Micro-Frequency Grids** (Suspicious): Pixel-perfect, high frequency, often in sky/noise (GAN artifacts)

```python
# BEFORE (AMBIGUOUS)
"Filter: IGNORE repetitive grid artifacts... caused by marching columns"

# AFTER (PRECISE)
"Filter: IGNORE MACRO-scale repetitive patterns (e.g., lines of soldiers, rows of tanks).
Focus: FLAG MICRO-scale perfect pixel-grid anomalies or symmetric star patterns in noise floor."
```

### Scientific Foundation

#### Error Level Analysis (ELA)
- **Purpose:** Detect compression level inconsistencies
- **Principle:** AI-generated images have uniform compression across entire image; real photos have varying compression (camera sensors compress differently across regions)
- **Signature:**
  - **Uniform rainbow static** â†’ AI-generated (consistent compression)
  - **Dark with edge noise** â†’ Real photograph (varying compression)
- **Metrics:**
  - **Variance Score:** `std_dev(ELA_pixel_values)`
    - **Low variance (<2.0)**: INCONCLUSIVE on social media (aggressive re-compression by platforms like WhatsApp/Facebook)
    - **High variance (â‰¥2.0)**: Potential manipulation/splicing
    - **VLM Guidance**: Focus on LOCAL inconsistencies (e.g., bright patch on dark background), NOT global uniformity

#### Fast Fourier Transform (FFT)
- **Purpose:** Detect frequency domain artifacts
- **Principle:** GANs and diffusion models introduce periodic patterns in frequency domain
- **Signatures:**
  - **Grid/Starfield patterns** â†’ AI-generated (GAN/Diffusion artifacts)
  - **Chaotic starburst** â†’ Real photograph (natural frequency distribution)
- **Preprocessing (CRITICAL - FIXED Dec 12, 2025):**
  1. **Center Crop + Padding** (NEVER resize - destroys high-frequency evidence)
     - If image â‰¥ 512Ã—512: Extract center 512Ã—512 square
     - If image < 512Ã—512: Pad with `cv2.BORDER_REFLECT`
  2. **Apply High-Pass Filter** (removes DC bias from natural lighting)
  3. **Mask DC + Axes** (removes social media "JPEG Cross" artifact)
     - Mask central DC component (5-pixel radius circle)
     - Mask horizontal/vertical axes through center (Â±1-2 pixels)
  4. **Dynamic peak threshold** based on OSINT context
- **Metrics:**
  - **Peak Detection:** Identify peaks above threshold (default: 20, adjusted by context)
  - **Pattern Classification:** Grid, Starfield, or Chaotic (Cross pattern = social media artifact, now masked)

#### EXIF Metadata Extraction (NEW)
- **Purpose:** Instant rejection for known AI generation tools
- **Principle:** AI tools often leave fingerprints in metadata fields
- **Auto-Fail Blacklist:**
  - **Tools:** Midjourney, Stable Diffusion, DALL-E, ComfyUI, Automatic1111, InvokeAI, Fooocus, Sora, Firefly
  - **Fields Checked:** Software, ImageDescription, UserComment, Make, Model
  - **Behavior:** If blacklisted tool detected â†’ Instant "Deepfake" verdict (confidence = 1.0)
- **Legitimate Signatures:**
  - Professional cameras (Canon, Nikon, Sony) â†’ Increase "Authentic" prior
  - Smartphone metadata (iPhone, Samsung) â†’ Neutral

### Module Specification: `forensics.py`

```python
"""
Forensic artifact generation for deepfake detection.
Generates ELA and FFT maps to expose AI generation signatures.
"""

import cv2
import numpy as np
from PIL import Image
import io
from typing import Union


class ArtifactGenerator:
    """Generate forensic artifacts for image authentication."""

    @staticmethod
    def generate_ela(
        image_input: Union[str, Image.Image, np.ndarray],
        quality: int = 90,
        scale_factor: int = 15
    ) -> bytes:
        """
        Generate Error Level Analysis (ELA) map.

        ELA highlights compression inconsistencies by comparing the original
        image to a recompressed version. Uniform compression (AI-generated)
        appears as rainbow static; varying compression (real photos) shows
        dark regions with edge noise.

        Args:
            image_input: Path to image, PIL Image, or numpy array
            quality: JPEG compression quality (default: 90)
            scale_factor: Amplification factor for visibility (default: 15)

        Returns:
            PNG-encoded bytes of the ELA map

        Algorithm:
            1. Load original image
            2. Compress to JPEG at specified quality
            3. Compute |Original - Compressed|
            4. Amplify differences: diff * scale_factor
            5. Normalize to 0-255 range
            6. Return as PNG bytes
        """
        # Load image
        if isinstance(image_input, str):
            original = cv2.imread(image_input)
        elif isinstance(image_input, Image.Image):
            original = cv2.cvtColor(np.array(image_input), cv2.COLOR_RGB2BGR)
        else:
            original = image_input

        # Compress to JPEG in memory
        encode_params = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
        _, compressed_bytes = cv2.imencode('.jpg', original, encode_params)

        # Decode compressed image
        compressed = cv2.imdecode(compressed_bytes, cv2.IMREAD_COLOR)

        # Compute absolute difference
        diff = cv2.absdiff(original, compressed).astype('float')

        # Scale for visibility
        ela = np.clip(diff * scale_factor, 0, 255).astype('uint8')

        # Convert to PNG bytes
        _, png_bytes = cv2.imencode('.png', ela)

        return png_bytes.tobytes()

    @staticmethod
    def generate_fft(
        image_input: Union[str, Image.Image, np.ndarray]
    ) -> bytes:
        """
        Generate Fast Fourier Transform (FFT) magnitude spectrum.

        FFT reveals frequency domain patterns. AI-generated images often
        exhibit grid, starfield, or cross patterns due to GAN/Diffusion
        architecture. Real photos show chaotic starburst patterns.

        Args:
            image_input: Path to image, PIL Image, or numpy array

        Returns:
            PNG-encoded bytes of the FFT magnitude spectrum

        Algorithm:
            1. Convert to grayscale
            2. Convert to float32
            3. Compute 2D DFT using cv2.dft
            4. Shift zero-frequency to center
            5. Compute magnitude spectrum: 20 * log(magnitude)
            6. Normalize to 0-255 range
            7. Return as PNG bytes
        """
        # Load and convert to grayscale
        if isinstance(image_input, str):
            gray = cv2.imread(image_input, cv2.IMREAD_GRAYSCALE)
        elif isinstance(image_input, Image.Image):
            gray = cv2.cvtColor(np.array(image_input), cv2.COLOR_RGB2GRAY)
        else:
            if len(image_input.shape) == 3:
                gray = cv2.cvtColor(image_input, cv2.COLOR_BGR2GRAY)
            else:
                gray = image_input

        # Convert to float32
        gray_float = np.float32(gray)

        # Compute DFT
        dft = cv2.dft(gray_float, flags=cv2.DFT_COMPLEX_OUTPUT)

        # Shift zero-frequency to center
        dft_shift = np.fft.fftshift(dft)

        # Compute magnitude spectrum
        magnitude = cv2.magnitude(dft_shift[:, :, 0], dft_shift[:, :, 1])

        # Apply log transform for visibility
        magnitude_log = 20 * np.log(magnitude + 1)  # +1 to avoid log(0)

        # Normalize to 0-255
        magnitude_normalized = cv2.normalize(
            magnitude_log, None, 0, 255, cv2.NORM_MINMAX
        )
        fft_image = np.uint8(magnitude_normalized)

        # Convert to PNG bytes
        _, png_bytes = cv2.imencode('.png', fft_image)

        return png_bytes.tobytes()
```

---

## Feature 2: Dynamic Model Configuration UI

### Rationale

**Problem:** Current model configuration is hardcoded in `config.py`, requiring code changes to add/modify models.

**Solution:** Provide a **Configuration Tab** in the Streamlit UI allowing users to:
1. Add model endpoints via text input (one at a time)
2. Bulk import model configurations via CSV upload
3. Edit/delete existing model configurations
4. Toggle models on/off without removing them
5. Test model connectivity before saving

### UI Specification: Tab 3 - Model Configuration

#### Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   âš™ï¸ Model Configuration                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Active Models: 4                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ âœ… Qwen3 VL 32B          â”‚ http://100.64.0.3:8006/v1/   â”‚   â”‚
â”‚  â”‚ âœ… InternVL 3.5 8B       â”‚ http://localhost:1234/v1/    â”‚   â”‚
â”‚  â”‚ âœ… MiniCPM-V 4.5         â”‚ http://100.64.0.3:8001/v1/   â”‚   â”‚
â”‚  â”‚ âœ… InternVL 2.5 8B       â”‚ http://100.64.0.1:8000/v1/   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ðŸ“ Add Model Manually                                  â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  Display Name:  [_________________________]             â”‚    â”‚
â”‚  â”‚  Base URL:      [_________________________]             â”‚    â”‚
â”‚  â”‚  Model Name:    [_________________________]             â”‚    â”‚
â”‚  â”‚  API Key:       [_________________________] (optional)  â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  [Test Connection]  [Add Model]                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ðŸ“¤ Bulk Import via CSV                                 â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  Upload CSV with columns:                               â”‚    â”‚
â”‚  â”‚  - display_name                                         â”‚    â”‚
â”‚  â”‚  - base_url                                             â”‚    â”‚
â”‚  â”‚  - model_name                                           â”‚    â”‚
â”‚  â”‚  - api_key (optional)                                   â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  [Choose File]  [Import Models]                        â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  [Download Template CSV]                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ðŸ“‹ Manage Existing Models                              â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚  [Table with: Name | URL | Status | Actions]            â”‚    â”‚
â”‚  â”‚  Qwen3 VL 32B   | http://100... | âœ… Online | [Edit][âŒ] â”‚    â”‚
â”‚  â”‚  InternVL 3.5   | http://local..| âœ… Online | [Edit][âŒ] â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  [ðŸ’¾ Save Configuration]  [ðŸ”„ Reset to Defaults]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Specification: `model_manager.py`

```python
"""
Dynamic model configuration management.
Handles adding, editing, deleting, and testing model endpoints.
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional
from openai import OpenAI


class ModelManager:
    """Manage model configurations dynamically."""

    CONFIG_FILE = "model_configs.json"

    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize ModelManager.

        Args:
            config_path: Path to JSON config file (default: model_configs.json)
        """
        self.config_path = Path(config_path or self.CONFIG_FILE)
        self.models = self._load_configs()

    def _load_configs(self) -> Dict:
        """Load model configurations from JSON file."""
        if self.config_path.exists():
            with open(self.config_path, 'r') as f:
                return json.load(f)
        else:
            # Return default configs from config.py
            return self._get_default_configs()

    def _get_default_configs(self) -> Dict:
        """Return default model configurations."""
        from config import MODEL_CONFIGS
        return MODEL_CONFIGS.copy()

    def save_configs(self):
        """Save current configurations to JSON file."""
        with open(self.config_path, 'w') as f:
            json.dump(self.models, f, indent=2)

    def add_model(
        self,
        model_key: str,
        display_name: str,
        base_url: str,
        model_name: str,
        api_key: str = "dummy",
        enabled: bool = True
    ) -> bool:
        """
        Add a new model configuration.

        Args:
            model_key: Unique identifier for the model
            display_name: Human-readable name
            base_url: OpenAI-compatible API endpoint
            model_name: Model identifier for API calls
            api_key: API key (default: "dummy" for vLLM)
            enabled: Whether model is active

        Returns:
            True if added successfully, False if key exists
        """
        if model_key in self.models:
            return False

        self.models[model_key] = {
            "display_name": display_name,
            "base_url": base_url,
            "model_name": model_name,
            "api_key": api_key,
            "enabled": enabled
        }
        self.save_configs()
        return True

    def remove_model(self, model_key: str) -> bool:
        """Remove a model configuration."""
        if model_key in self.models:
            del self.models[model_key]
            self.save_configs()
            return True
        return False

    def update_model(self, model_key: str, **kwargs) -> bool:
        """Update model configuration fields."""
        if model_key not in self.models:
            return False

        self.models[model_key].update(kwargs)
        self.save_configs()
        return True

    def test_connection(self, model_key: str) -> Dict:
        """
        Test connection to a model endpoint.

        Returns:
            {
                "success": bool,
                "message": str,
                "latency_ms": float (if success)
            }
        """
        import time

        if model_key not in self.models:
            return {"success": False, "message": "Model not found"}

        config = self.models[model_key]

        try:
            client = OpenAI(
                base_url=config["base_url"],
                api_key=config.get("api_key", "dummy")
            )

            start = time.time()

            # Simple test call
            response = client.chat.completions.create(
                model=config["model_name"],
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=1,
                temperature=0.0
            )

            latency = (time.time() - start) * 1000  # Convert to ms

            return {
                "success": True,
                "message": "Connection successful",
                "latency_ms": round(latency, 2)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"Connection failed: {str(e)}"
            }

    def import_from_csv(self, csv_path: str) -> Dict:
        """
        Import model configurations from CSV.

        CSV format:
            display_name,base_url,model_name,api_key

        Returns:
            {
                "success": int (count of successful imports),
                "failed": List[str] (failed model names),
                "errors": List[str] (error messages)
            }
        """
        df = pd.read_csv(csv_path)

        required_cols = {"display_name", "base_url", "model_name"}
        if not required_cols.issubset(df.columns):
            missing = required_cols - set(df.columns)
            return {
                "success": 0,
                "failed": [],
                "errors": [f"Missing required columns: {missing}"]
            }

        success_count = 0
        failed_names = []
        errors = []

        for idx, row in df.iterrows():
            # Generate model_key from display_name
            model_key = row['display_name'].replace(" ", "_").replace("-", "_")

            api_key = row.get('api_key', 'dummy')
            if pd.isna(api_key):
                api_key = 'dummy'

            try:
                added = self.add_model(
                    model_key=model_key,
                    display_name=row['display_name'],
                    base_url=row['base_url'],
                    model_name=row['model_name'],
                    api_key=api_key
                )

                if added:
                    success_count += 1
                else:
                    failed_names.append(row['display_name'])
                    errors.append(f"{row['display_name']}: Model key already exists")

            except Exception as e:
                failed_names.append(row['display_name'])
                errors.append(f"{row['display_name']}: {str(e)}")

        return {
            "success": success_count,
            "failed": failed_names,
            "errors": errors
        }

    def get_enabled_models(self) -> Dict:
        """Return only enabled model configurations."""
        return {
            key: config
            for key, config in self.models.items()
            if config.get("enabled", True)
        }

    def export_template_csv(self, output_path: str = "model_import_template.csv"):
        """Export a template CSV for bulk import."""
        template_df = pd.DataFrame([
            {
                "display_name": "Example Model",
                "base_url": "http://localhost:8000/v1/",
                "model_name": "model-name-here",
                "api_key": "your-api-key-or-dummy"
            }
        ])
        template_df.to_csv(output_path, index=False)
        return output_path
```

---

## Feature 3: Hybrid Classification System (Three-Tier)

### Rationale

**Problem:** Binary classification (Real vs Fake) forces hard decisions on borderline cases where the model is genuinely uncertain. This causes:
- False positives (legitimate post-processed photos flagged as fake)
- Reduced trust in suspicious-but-ambiguous cases

**Solution:** Implement a **three-tier classification system** with logit-calibrated confidence scores:
- **Authentic** (P_fake < 0.50): High confidence the image is a real photograph
- **Suspicious** (0.50 â‰¤ P_fake < 0.90): Borderline case requiring human review
- **Deepfake** (P_fake â‰¥ 0.90): High confidence the image is AI-generated

This approach:
1. Reduces false positive rate by surfacing uncertainty
2. Provides actionable triage for human analysts
3. Maintains high precision on confident predictions

### Three-Tier Classification Thresholds

| Tier | Condition | P(Fake) Range | Interpretation |
|------|-----------|---------------|----------------|
| **Authentic** | P_fake < 0.50 | 0.00 - 0.49 | High confidence in real photograph |
| **Suspicious** | 0.50 â‰¤ P_fake < 0.90 | 0.50 - 0.89 | Uncertain, requires human review |
| **Deepfake** | P_fake â‰¥ 0.90 | 0.90 - 1.00 | High confidence in AI generation |

### Workflow Integration

The three-tier system integrates with the two-stage prompting pipeline:

```
Stage 0: Metadata Check
    â†“ (if auto-fail detected)
    â”œâ”€â†’ Instant "Deepfake" (confidence = 1.0)
    â†“ (if no auto-fail)
Stage 1: Forensic Analysis
    â†“
Stage 2: VLM Analysis (Request 1)
    â†“
Stage 3: Verdict Extraction (Request 2)
    â†“
Stage 4: Three-Tier Classification
    â”œâ”€â†’ P_fake < 0.50: "Authentic"
    â”œâ”€â†’ 0.50 â‰¤ P_fake < 0.90: "Suspicious"
    â””â”€â†’ P_fake â‰¥ 0.90: "Deepfake"
```

### Module Integration Notes

**NOTE:** The three-tier classification logic is implemented in the `OSINTDetector` class within [detector.py](#module-specification-detectorpy) (see `_classify_tier()` method).

The standalone `ForensicClassifier` (classifier.py) is **deprecated** in favor of the unified `OSINTDetector` which integrates:
- Metadata extraction (Stage 0)
- Forensic artifact generation (Stage 1)
- Two-stage prompting (Stages 2-3)
- Three-tier classification (Stage 4)
- OSINT-aware context injection

For backward compatibility with existing batch evaluation code, the old `classifier.py` MCQ-based approach remains functional but does not include OSINT enhancements.

---

## Data Flow

### Detection Flow (Tab 1) - OSINT Mode

```
User uploads image + selects OSINT context (auto/military/disaster/propaganda)
       â†“
[Stage 0: Metadata Extraction]
OSINTDetector extracts EXIF metadata (detector.py)
       â†“
Check for AI tool signatures (Midjourney, Stable Diffusion, etc.)
       â†“
       â”œâ”€â†’ If blacklisted tool detected:
       â”‚   â””â”€â†’ Instant "Deepfake" verdict (confidence = 1.0)
       â”‚       â””â”€â†’ DONE
       â†“ (if no auto-fail)
[Stage 1: Forensic Analysis]
Generate ELA map (forensics.py)
Generate FFT spectrum (forensics.py)
Build forensic report with EXIF + ELA + FFT metrics
       â†“
[Stage 2: VLM Analysis - Request 1]
Send system prompt with OSINT protocol (Case A/B/C)
Send forensic report + 3 images (original + ELA + FFT)
VLM returns detailed reasoning (max 300 tokens)
Store response in conversation history
       â†“
[Stage 3: Verdict Extraction - Request 2] (KV-Cache Optimized)
Append verdict MCQ prompt to same conversation
VLM returns single token: A (Real) or B (Fake)
Extract logprobs for calibrated confidence
       â†“
[Stage 4: Three-Tier Classification]
       â”œâ”€â†’ P_fake < 0.50: "Authentic"
       â”œâ”€â†’ 0.50 â‰¤ P_fake < 0.90: "Suspicious"
       â””â”€â†’ P_fake â‰¥ 0.90: "Deepfake"
       â†“
Display: Tier + Confidence + Reasoning + Forensic Report + Artifacts
```

### Evaluation Flow (Tab 2)

```
User uploads batch images + ground truth CSV
       â†“
Select models from configured endpoints
       â†“
For each image:
  â”œâ”€ Run OSINTDetector.detect() (full 4-stage pipeline)
  â”œâ”€ OR: Use legacy ForensicClassifier for compatibility
  â””â”€ Record: tier, confidence, reasoning, ground truth
       â†“
Calculate metrics: Accuracy, Precision, Recall, F1
  - Three-tier metrics: Authentic vs Suspicious vs Deepfake
  - Binary metrics: (Authentic + Suspicious) vs Deepfake
       â†“
Generate confusion matrices per model
       â†“
Export Excel + visualizations
```

### Configuration Flow (Tab 3)

```
User adds model (manual or CSV)
       â†“
Test connection (optional)
       â†“
Save to model_configs.json
       â†“
Model appears in Tab 1/2 dropdowns
```

---

## API Specifications

### OpenAI-Compatible Endpoint Requirements

All model endpoints must support:

```python
# Standard chat completion
response = client.chat.completions.create(
    model="model-name",
    messages=[...],
    temperature=0.0,
    max_tokens=1,
    logprobs=True,      # REQUIRED for logit calibration
    top_logprobs=5      # REQUIRED (minimum 5)
)

# Response structure
response.choices[0].logprobs.content[0].top_logprobs
# Must return list of objects with:
# - token: str
# - logprob: float
```

**Supported APIs:**
- vLLM (local)
- OpenAI GPT-4o / GPT-4 Turbo
- Google Gemini (via OpenAI compatibility layer)
- Any custom API implementing OpenAI spec

---

## UI/UX Specifications

### Tab 1: Detection (OSINT Mode with Debug Features)

**New Controls:**
1. **OSINT Context Selector** (dropdown)
   - Options: `Auto-Detect`, `Military`, `Disaster/HADR`, `Propaganda/Showcase`
   - Default: `Auto-Detect`
   - Help text: "Select scene type for context-adaptive forensic thresholds"

2. **Debug Mode Toggle** (checkbox)
   - Label: "ðŸ” Enable Debug Mode"
   - Default: `False`
   - Help text: "Show detailed forensic reports, VLM reasoning, and raw logprobs"

**Standard Output Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Detection Results                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Tier: SUSPICIOUS                                            â”‚
â”‚  Confidence: 87.3%  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘]                â”‚
â”‚                      â†‘                                       â”‚
â”‚                  Color-coded:                                â”‚
â”‚                  Green (<50%) = Authentic                    â”‚
â”‚                  Yellow (50-90%) = Suspicious                â”‚
â”‚                  Red (â‰¥90%) = Deepfake                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Original Image  â”‚  ELA Map      â”‚  FFT Spectrum     â”‚   â”‚
â”‚  â”‚  [Image]         â”‚  [Image]      â”‚  [Image]          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  VLM Reasoning:                                              â”‚
â”‚  "Based on the forensic evidence, I observe uniform          â”‚
â”‚   compression in the ELA map suggesting AI generation.       â”‚
â”‚   However, the FFT spectrum shows some natural chaos..."     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Debug Mode Output Layout:**

When debug mode is enabled, additional expandable sections appear below the standard output:

```
â–¼ ðŸ”¬ Forensic Lab Report (Raw Data)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ EXIF Metadata:                                           â”‚
  â”‚   Make: Canon                                            â”‚
  â”‚   Model: EOS R5                                          â”‚
  â”‚   Software: Adobe Photoshop 2024                         â”‚
  â”‚   DateTime: 2024:12:10 14:23:45                          â”‚
  â”‚   ISO: 400                                               â”‚
  â”‚   FNumber: f/2.8                                         â”‚
  â”‚   âš ï¸ Auto-Fail Check: PASSED (no AI tools detected)      â”‚
  â”‚                                                           â”‚
  â”‚ ELA Analysis:                                            â”‚
  â”‚   Variance Score: 1.85                                   â”‚
  â”‚   Interpretation: Low variance â†’ Uniform compression     â”‚
  â”‚   Threshold: <2.0 (AI indicator)                         â”‚
  â”‚   Visual Pattern: Rainbow static across entire image     â”‚
  â”‚                                                           â”‚
  â”‚ FFT Analysis:                                            â”‚
  â”‚   Pattern Type: Chaotic Starburst                        â”‚
  â”‚   Peak Threshold (Base): 20                              â”‚
  â”‚   Peak Threshold (Adjusted): 24 (+20% for military)      â”‚
  â”‚   Peaks Detected: 3 above threshold                      â”‚
  â”‚   Interpretation: Mixed signals (some GAN artifacts)     â”‚
  â”‚                                                           â”‚
  â”‚ OSINT Context Applied: Military (Case A)                 â”‚
  â”‚   Dynamic Adjustments:                                   â”‚
  â”‚   - FFT threshold increased by +20%                      â”‚
  â”‚   - Ignoring repetitive grid patterns                    â”‚
  â”‚   - Focusing on clone stamp errors                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â–¼ ðŸ§  VLM Analysis Output (Stage 2 - Request 1)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Full Reasoning Text:                                     â”‚
  â”‚                                                           â”‚
  â”‚ "Based on the forensic lab report, I observe:            â”‚
  â”‚                                                           â”‚
  â”‚ 1. EXIF Metadata: Image was edited in Adobe Photoshop,   â”‚
  â”‚    which is common for professional photography but      â”‚
  â”‚    could also indicate manipulation. No AI tool          â”‚
  â”‚    signatures detected.                                  â”‚
  â”‚                                                           â”‚
  â”‚ 2. ELA Map: Shows uniform rainbow static across the      â”‚
  â”‚    entire image, indicating consistent compression       â”‚
  â”‚    levels. This is typical of AI-generated images.       â”‚
  â”‚                                                           â”‚
  â”‚ 3. FFT Spectrum: Displays a chaotic starburst pattern    â”‚
  â”‚    with some periodic peaks. The peaks suggest possible  â”‚
  â”‚    GAN artifacts, but given the military context, some   â”‚
  â”‚    repetition is expected from uniform formations.       â”‚
  â”‚                                                           â”‚
  â”‚ 4. Context: Military parade scene with uniform grid      â”‚
  â”‚    formations. Applying Case A protocol - ignoring       â”‚
  â”‚    some grid artifacts as they could be legitimate.      â”‚
  â”‚                                                           â”‚
  â”‚ Conclusion: The ELA uniformity is concerning, but the    â”‚
  â”‚ professional camera metadata and context-adjusted FFT    â”‚
  â”‚ analysis create uncertainty."                            â”‚
  â”‚                                                           â”‚
  â”‚ API Call Metadata:                                       â”‚
  â”‚   Model: Qwen3-VL-32B-Instruct                           â”‚
  â”‚   Endpoint: http://100.64.0.3:8006/v1/                   â”‚
  â”‚   Tokens Sent: 2,347 (3 images + forensic report)        â”‚
  â”‚   Tokens Generated: 287                                  â”‚
  â”‚   Latency: 2.41 seconds                                  â”‚
  â”‚   Temperature: 0.0 (deterministic)                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â–¼ ðŸ“Š Logprobs & Verdict Extraction (Stage 3 - Request 2)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Top K=5 Tokens (Raw Output):                             â”‚
  â”‚                                                           â”‚
  â”‚   Rank â”‚ Token â”‚ Logprob  â”‚ Probability â”‚ Interpretationâ”‚
  â”‚   â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚    1   â”‚  'B'  â”‚  -0.142  â”‚   0.8677    â”‚ FAKE          â”‚
  â”‚    2   â”‚  'A'  â”‚  -2.051  â”‚   0.1287    â”‚ REAL          â”‚
  â”‚    3   â”‚  '\n' â”‚  -5.234  â”‚   0.0053    â”‚ (newline)     â”‚
  â”‚    4   â”‚  'b'  â”‚  -6.891  â”‚   0.0010    â”‚ FAKE (lower)  â”‚
  â”‚    5   â”‚ 'The' â”‚  -8.123  â”‚   0.0003    â”‚ (spurious)    â”‚
  â”‚                                                           â”‚
  â”‚ Softmax Normalization (A vs B only):                     â”‚
  â”‚   P(Fake) = 0.8677 / (0.8677 + 0.1287) = 0.8710          â”‚
  â”‚   P(Real) = 0.1287 / (0.8677 + 0.1287) = 0.1290          â”‚
  â”‚                                                           â”‚
  â”‚ Three-Tier Classification:                               â”‚
  â”‚   Confidence (P_fake): 0.8710 (87.1%)                    â”‚
  â”‚   Threshold Check:                                       â”‚
  â”‚     - P_fake < 0.50? NO                                  â”‚
  â”‚     - P_fake â‰¥ 0.90? NO                                  â”‚
  â”‚   â†’ TIER: SUSPICIOUS (requires human review)             â”‚
  â”‚                                                           â”‚
  â”‚ API Call Metadata:                                       â”‚
  â”‚   Model: Qwen3-VL-32B-Instruct                           â”‚
  â”‚   Endpoint: http://100.64.0.3:8006/v1/                   â”‚
  â”‚   Tokens Sent: 12 (verdict prompt only - reusing cache)  â”‚
  â”‚   Tokens Generated: 1 (single token: 'B')                â”‚
  â”‚   Latency: 0.23 seconds (90.5% reduction!)               â”‚
  â”‚   Temperature: 0.0 (deterministic)                       â”‚
  â”‚   KV-Cache Hit: âœ… YES (image tokens reused)             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â–¼ âš™ï¸ System Prompt (Context-Adaptive)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Full System Prompt Sent to Model:                        â”‚
  â”‚                                                           â”‚
  â”‚ "You are a Senior OSINT Image Analyst specializing in    â”‚
  â”‚ military, disaster, and propaganda imagery verification. â”‚
  â”‚                                                           â”‚
  â”‚ CASE A: Military Context (Uniforms/Parades/Formations)   â”‚
  â”‚ - Filter: IGNORE repetitive grid artifacts in FFT        â”‚
  â”‚   (formations create patterns).                          â”‚
  â”‚ - Focus: Look for clone stamp errors (duplicate faces,   â”‚
  â”‚   floating weapons).                                     â”‚
  â”‚ - Threshold: FFT peak threshold increased by +20%.       â”‚
  â”‚                                                           â”‚
  â”‚ [Additional Case B and Case C protocols included when    â”‚
  â”‚  context is set to 'Auto-Detect']"                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â–¼ ðŸ–¼ï¸ Forensic Artifacts (High Resolution)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ [Full-resolution downloadable versions]                  â”‚
  â”‚                                                           â”‚
  â”‚  [Download Original] [Download ELA] [Download FFT]       â”‚
  â”‚                                                           â”‚
  â”‚  Original Image (1920x1080)                              â”‚
  â”‚  [Larger preview with zoom capability]                   â”‚
  â”‚                                                           â”‚
  â”‚  ELA Map (1920x1080)                                     â”‚
  â”‚  [Color-coded heatmap with legend]                       â”‚
  â”‚  Legend: Dark = consistent compression                   â”‚
  â”‚          Bright = compression anomalies                  â”‚
  â”‚                                                           â”‚
  â”‚  FFT Spectrum (512x512 - standardized)                   â”‚
  â”‚  [Frequency domain visualization]                        â”‚
  â”‚  Legend: Center = low frequency                          â”‚
  â”‚          Edges = high frequency                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â–¼ â±ï¸ Performance Metrics
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Stage-by-Stage Timing:                                   â”‚
  â”‚                                                           â”‚
  â”‚   Stage 0 (Metadata): 0.04s                              â”‚
  â”‚   Stage 1 (Forensics): 0.87s                             â”‚
  â”‚     â”œâ”€ ELA generation: 0.43s                             â”‚
  â”‚     â””â”€ FFT generation: 0.44s                             â”‚
  â”‚   Stage 2 (VLM Analysis): 2.41s                          â”‚
  â”‚   Stage 3 (Verdict): 0.23s (âš¡ 90% faster via cache)     â”‚
  â”‚   Stage 4 (Classification): <0.01s                       â”‚
  â”‚                                                           â”‚
  â”‚   Total Pipeline: 3.55 seconds                           â”‚
  â”‚                                                           â”‚
  â”‚ Cost Estimation (if using paid API):                     â”‚
  â”‚   Request 1: ~2,347 tokens @ $0.01/1K = $0.023           â”‚
  â”‚   Request 2: ~12 tokens @ $0.01/1K = $0.0001             â”‚
  â”‚   Total: ~$0.023 per image                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Notes:**

1. **Debug Toggle State:** Persist across detections using `st.session_state`
2. **Expandable Sections:** Use `st.expander()` for each debug section
3. **Color Coding:**
   - Authentic: Green progress bar (`st.success()`)
   - Suspicious: Yellow/Orange progress bar (`st.warning()`)
   - Deepfake: Red progress bar (`st.error()`)
4. **Download Buttons:** Use `st.download_button()` for forensic artifacts
5. **Code Blocks:** Use `st.code()` for displaying system prompts and raw data

### Tab 2: Evaluation (Minor Updates)

**Changes:**
- Model selector now pulls from `ModelManager.get_enabled_models()`
- Add "Refresh Models" button to reload from config
- Display confidence scores in results table

### Tab 3: Configuration (New)

See [Feature 2 UI Specification](#ui-specification-tab-3---model-configuration)

---

## Security & Best Practices

### Docker Security

**Non-Root User Execution (CRITICAL)**

Containers should never run as root to limit security impact of potential compromises.

```dockerfile
# Create non-root user
RUN useradd -m -s /bin/bash --uid 1001 appuser

# Set ownership of files
COPY --chown=appuser:appuser app.py .
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser
```

**Benefits:**
- Limits blast radius if container is compromised
- Prevents privilege escalation attacks
- Follows principle of least privilege
- Required for many enterprise security policies

### Code Quality

**Temperature Settings**

- **Forensic Analysis:** Use `temperature=0.0` for deterministic, reproducible results
- **Chat Interactions:** Use `temperature=0.7` to allow natural conversation
- **Rationale:** Forensic analysis requires consistency across runs; chat benefits from variability

```python
# Forensic classification
response = client.chat.completions.create(
    model=model_name,
    messages=[...],
    temperature=0.0  # Deterministic for forensic analysis
)

# Chat interaction
response = client.chat.completions.create(
    model=model_name,
    messages=[...],
    temperature=0.7  # Allow creativity for chat
)
```

**Import Style (PEP 8 Compliance)**

Follow PEP 8 style guide for clean, readable code:

```python
# Bad
import io, tempfile, cv2, pandas as pd

# Good
import io
import tempfile
import cv2
import pandas as pd
```

**Resource Cleanup**

Always clean up temporary resources using try/finally blocks:

```python
# Temporary file handling
tmp_path = None
try:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(data)
        tmp_path = tmp.name
    # Process file
    process_video(tmp_path)
finally:
    # Ensure cleanup even if errors occur
    if tmp_path and os.path.exists(tmp_path):
        try:
            os.unlink(tmp_path)
        except Exception:
            pass  # Ignore cleanup errors
```

### Performance Optimization

**DataFrame Lookups**

Convert DataFrame columns to sets for O(1) lookup instead of O(N):

```python
# Bad: O(N) lookup inside loop
for img_file in eval_images:
    filename = img_file.name
    if filename not in gt_df["filename"].values:  # O(N) lookup
        continue

# Good: O(1) lookup with set
gt_filenames = set(gt_df["filename"].values)  # One-time conversion
for img_file in eval_images:
    filename = img_file.name
    if filename not in gt_filenames:  # O(1) lookup
        continue
```

**Impact:** For dataset with N images, reduces complexity from O(NÂ²) to O(N).

### Configuration Management

**Environment Variables vs Hardcoding**

The `model_manager.py` module supersedes hardcoded configurations by providing:
- Dynamic model addition/removal via UI
- Persistent JSON-based configuration
- No code changes required for new models
- Support for environment-specific endpoints

**Rationale:** While environment variables are useful for deployment-time configuration, the ModelManager provides runtime flexibility without redeployment.

### Documentation Standards

**Docker Compose v2 Syntax**

Use modern `docker compose` (v2) syntax instead of legacy `docker-compose` (v1):

```bash
# Modern syntax (v2)
docker compose up --build
docker compose logs -f
docker compose down

# Legacy syntax (v1) - avoid
docker-compose up --build
```

**Benefits:**
- v2 is integrated into Docker CLI (no separate installation)
- Improved performance and features
- Better compatibility with modern Docker versions

### Repository Hygiene

**Remove Obsolete Files**

- Keep only actively used scripts (e.g., `generate_report_updated.py`)
- Remove deprecated versions (e.g., `generate_report.py`)
- Reduces confusion and maintenance burden
- Keeps repository clean and navigable

**Package Management**

- Remove duplicate dependencies in Dockerfile
- Use `--no-install-recommends` for minimal image size
- Clean apt cache after installation

```dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends \
    libglib2.0-0 \
    # No duplicate entries
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*
```

---

## Testing & Validation

### Unit Tests

**forensics.py:**
- Test ELA generation with known AI/real images
- Test FFT generation with synthetic grid patterns
- Verify output format (PNG bytes)

**classifier.py:**
- Test logprob parsing with mock responses
- Test token variation handling (REAL vs real vs Real)
- Test threshold application

**model_manager.py:**
- Test add/remove/update operations
- Test CSV import with valid/invalid data
- Test connection testing with mock endpoints

### Integration Tests

**End-to-End Detection:**
1. Upload known AI image â†’ Expect "AI-Generated" with high confidence
2. Upload known real photo â†’ Expect "Authentic" with high confidence
3. Upload edge case â†’ Verify forensic artifacts visible

**Batch Evaluation:**
1. Run on validation set with ground truth
2. Verify metrics match manual calculation
3. Test with multiple models simultaneously

### Validation Criteria

**Forensic Artifacts:**
- ELA maps visually distinct for AI vs real images
- FFT spectra show expected patterns (grid for AI, starburst for real)

**Calibration:**
- Confidence scores correlate with accuracy (well-calibrated)
- Histogram of scores shows separation between classes

**Model Management:**
- All OpenAI-compatible endpoints work correctly
- CSV import handles errors gracefully

---

## Implementation Modules

### Module Architecture

```
deepfake-detection/
â”œâ”€â”€ app.py                        # Streamlit UI (3 tabs)
â”œâ”€â”€ config.py                     # Base model configurations
â”œâ”€â”€ shared_functions.py           # Common utilities
â”‚
â”œâ”€â”€ forensics.py                  # âœ… IMPLEMENTED (Phase 1)
â”‚   â””â”€â”€ ArtifactGenerator         # ELA + FFT generation
â”‚
â”œâ”€â”€ detector.py                   # ðŸ†• NEW (OSINT-specific)
â”‚   â””â”€â”€ OSINTDetector             # Two-stage detection pipeline
â”‚       â”œâ”€â”€ Metadata extraction
â”‚       â”œâ”€â”€ Forensic generation
â”‚       â”œâ”€â”€ Two-stage API calls
â”‚       â””â”€â”€ Three-tier classification
â”‚
â”œâ”€â”€ classifier.py                 # âœ… LEGACY (backward compat)
â”‚   â””â”€â”€ ForensicClassifier        # MCQ-based logit calibration
â”‚
â”œâ”€â”€ model_manager.py              # Phase 2
â”‚   â””â”€â”€ ModelManager              # Dynamic model configuration
â”‚
â””â”€â”€ generate_report_updated.py   # Excel report generation
```

### Module Status

| Module | Status | Phase | OSINT Support | Notes |
|--------|--------|-------|---------------|-------|
| **forensics.py** | âœ… Implemented | 1 | Partial | ELA/FFT working; needs metadata extraction + FFT preprocessing |
| **detector.py** | ðŸ†• To Implement | 1 | Full | Complete OSINT pipeline with two-stage prompting |
| **classifier.py** | âœ… Implemented | 1 | None | Legacy MCQ approach; use for batch eval compatibility |
| **model_manager.py** | â³ Planned | 2 | N/A | Dynamic model management UI |
| **app.py** | ðŸ”„ Needs Update | 1-2 | Partial | Add OSINT context selector + three-tier display |

### Dependency Requirements

**New Dependencies for OSINT Features:**
```bash
pip install exifread  # EXIF metadata extraction
```

**All Dependencies:**
```
streamlit>=1.28.0
pillow>=10.0.0
opencv-python-headless
numpy>=1.24.0
pandas>=2.0.0
openai>=1.0.0
xlsxwriter>=3.1.0
exifread>=3.0.0  # NEW
```

---

## Implementation Roadmap

### Phase 1: OSINT-Specific Detection (Week 1) âš¡ **PRIORITY**

**Goal:** Implement complete OSINT-aware detection pipeline with two-stage prompting

- [ ] **Update forensics.py** (Est: 2 hours)
  - [ ] Add `extract_metadata()` function using exifread
  - [ ] Add `compute_ela_variance()` metric
  - [ ] Add FFT preprocessing (512x512 resize, high-pass filter)
  - [ ] Add dynamic threshold adjustment based on context

- [ ] **Create detector.py** (Est: 4 hours)
  - [ ] Implement `OSINTDetector` class
  - [ ] Metadata auto-fail logic with blacklist
  - [ ] Two-stage API calling pattern (Request 1 + 2)
  - [ ] Context-adaptive system prompt generation
  - [ ] Three-tier classification logic
  - [ ] Comprehensive error handling

- [ ] **Update app.py Tab 1** (Est: 4 hours)
  - [ ] Add OSINT context selector: auto/military/disaster/propaganda
  - [ ] Add debug mode toggle checkbox
  - [ ] Replace legacy detection with `OSINTDetector.detect(debug=debug_mode)`
  - [ ] Display three-tier result (Authentic/Suspicious/Deepfake)
  - [ ] Show VLM reasoning from Stage 2
  - [ ] Display forensic report with EXIF + ELA + FFT metrics
  - [ ] Add confidence visualization (progress bar with color coding)
  - [ ] Implement debug mode expandable sections:
    - [ ] Forensic Lab Report (raw EXIF, ELA variance, FFT metrics)
    - [ ] VLM Analysis Output (full reasoning + API metadata)
    - [ ] Logprobs & Verdict (top K=5 table + softmax calculation)
    - [ ] System Prompt (context-adaptive prompt display)
    - [ ] Forensic Artifacts (high-res downloadable versions)
    - [ ] Performance Metrics (stage timing + cost estimation)

- [ ] **Testing** (Est: 2 hours)
  - [ ] Test with known AI images (should auto-fail on metadata)
  - [ ] Test with real photos (should classify correctly)
  - [ ] Test military/disaster/propaganda contexts
  - [ ] Verify KV-cache optimization (measure Request 2 latency)
  - [ ] Test debug mode with all expandable sections
  - [ ] Verify performance metrics accuracy

**Total Phase 1 Estimate:** 12 hours (increased from 11 due to debug features)

### Phase 2: Model Management (Week 1-2)

- [ ] Implement `model_manager.py` (Est: 3 hours)
- [ ] Create Tab 3 UI (configuration interface) (Est: 3 hours)
- [ ] Add manual model addition with connection testing (Est: 2 hours)
- [ ] Add CSV bulk import (Est: 2 hours)
- [ ] Integrate with Tabs 1 & 2 model selectors (Est: 2 hours)

**Total Phase 2 Estimate:** 12 hours

### Phase 3: Batch Evaluation Updates (Week 2)

- [ ] **Update app.py Tab 2** (Est: 4 hours)
  - [ ] Add OSINT context selector for batch mode
  - [ ] Support three-tier ground truth CSV (Authentic/Suspicious/Deepfake)
  - [ ] Display tier-specific metrics
  - [ ] Add reasoning column to results table
  - [ ] Export forensic reports for each image

- [ ] **Update generate_report_updated.py** (Est: 2 hours)
  - [ ] Add three-tier confusion matrix
  - [ ] Add confidence calibration plots
  - [ ] Add per-context performance breakdown

**Total Phase 3 Estimate:** 6 hours

### Phase 4: Advanced Features (Week 3)

- [ ] **Enhanced Forensics** (Est: 4 hours)
  - [ ] FFT peak detection algorithm
  - [ ] Pattern classification (Grid/Cross/Starfield/Chaotic)
  - [ ] Context-adaptive threshold adjustment
  - [ ] Watermark detection mode

- [ ] **Performance Optimization** (Est: 2 hours)
  - [ ] Parallel forensic generation (ELA + FFT)
  - [ ] Batch API calls for evaluation
  - [ ] Response caching

**Total Phase 4 Estimate:** 6 hours

### Phase 5: Testing & Documentation (Week 3-4)

- [ ] **Unit Tests** (Est: 4 hours)
  - [ ] Test forensics.py metadata extraction
  - [ ] Test detector.py two-stage pipeline
  - [ ] Test three-tier classification thresholds
  - [ ] Mock API responses for deterministic testing

- [ ] **Integration Tests** (Est: 3 hours)
  - [ ] End-to-end detection with real models
  - [ ] Batch evaluation with validation dataset
  - [ ] Model management workflows

- [ ] **Documentation** (Est: 3 hours)
  - [ ] Update README with OSINT features
  - [ ] Create user guide for context selection
  - [ ] Document API requirements (logprobs support)
  - [ ] Add troubleshooting guide

**Total Phase 5 Estimate:** 10 hours

---

## Overall Timeline

| Phase | Duration | Priority | Dependencies |
|-------|----------|----------|--------------|
| Phase 1 | 12 hours | **HIGH** | None |
| Phase 2 | 12 hours | Medium | None (can run parallel with Phase 1) |
| Phase 3 | 6 hours | Medium | Phase 1 (detector.py) |
| Phase 4 | 6 hours | Low | Phase 1 (forensics.py) |
| Phase 5 | 10 hours | Medium | All phases |

**Total Estimated Effort:** 46 hours (~1 week of full-time work)

---

## Appendix

### CSV Import Template

```csv
display_name,base_url,model_name,api_key
GPT-4o,https://api.openai.com/v1/,gpt-4o,sk-your-key-here
Qwen3 VL Local,http://localhost:8000/v1/,Qwen3-VL-32B-Instruct,dummy
Gemini Pro Vision,https://generativelanguage.googleapis.com/v1beta/,gemini-pro-vision,your-gemini-key
```

### Configuration File Schema (`model_configs.json`)

```json
{
  "model_key": {
    "display_name": "Human-readable name",
    "base_url": "https://api.endpoint.com/v1/",
    "model_name": "model-identifier",
    "api_key": "api-key-or-dummy",
    "enabled": true
  }
}
```

### Environment Variables

```bash
# Optional: Set default API keys
export OPENAI_API_KEY="sk-..."
export GEMINI_API_KEY="..."

# Optional: Set default model config path
export MODEL_CONFIG_PATH="/path/to/model_configs.json"
```

---

## Version History

**v2.1 (2025-12-11)** - OSINT Specialization Update
- âœ… Added OSINT-Specific Enhancements section (Case A/B/C protocols)
- âœ… Added Two-Stage Prompting Strategy (KV-cache optimization)
- âœ… Added `detector.py` module specification (complete OSINT pipeline)
- âœ… Updated Feature 1 with metadata extraction (EXIF auto-fail)
- âœ… Updated Feature 3 to three-tier classification (Authentic/Suspicious/Deepfake)
- âœ… Added ELA variance metrics and FFT preprocessing specifications
- âœ… Updated data flows for OSINT detection pipeline
- âœ… Added Implementation Modules section with status tracking
- âœ… Restructured roadmap into 5 phases with effort estimates
- âœ… Deprecated standalone `classifier.py` in favor of unified `OSINTDetector`
- âœ… Added comprehensive Debug Mode UI specification with 6 expandable sections
- âœ… Added debug parameter to detector.py with full metadata tracking
- âœ… Added performance metrics and cost estimation display

**v2.0 (2025-12-11)** - Forensic Foundation
- Added forensic scanner design (ELA/FFT)
- Added logit calibration system
- Added dynamic model configuration UI
- Consolidated all feature specs into unified document
- Addressed all code review comments from Gemini PR #1
- Implemented Docker security hardening (non-root user)

**v1.0 (2025-11-26)** - Initial Release
- Initial batch evaluation system
- Docker containerization
- Report generation

---

## Document Status

**Status:** âœ… **Design Complete - Ready for Phase 1 Implementation**

**Next Steps:**
1. **Immediate:** Install `exifread` dependency
2. **Phase 1 (11 hours):**
   - Update `forensics.py` with metadata extraction
   - Create `detector.py` with OSINT pipeline
   - Update `app.py` Tab 1 with three-tier UI
3. **Validation:** Test with military/disaster/propaganda imagery

**Key Design Decisions Finalized:**
- âœ… OSINT context-adaptive analysis protocols
- âœ… Two-stage prompting for KV-cache optimization
- âœ… Three-tier classification system (Authentic/Suspicious/Deepfake)
- âœ… Metadata auto-fail for known AI tools
- âœ… MCQ verdict format (A/B) for logprob reliability
- âœ… Backward compatibility via legacy `classifier.py`

**Design Spec Completeness:** 100% (all sections updated)
**Architecture Pattern:** "Cyborg" (Signal Processing + Semantic Reasoning)
**Target Use Case:** OSINT image analysis (Military/Disaster/Propaganda)
